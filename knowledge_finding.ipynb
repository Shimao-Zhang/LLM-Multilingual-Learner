{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nfs03/anaconda3/envs/zhMulti/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, input, tokenizer, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = tokenizer(input, return_tensors='pt')\n",
    "        output = model(input.input_ids.to(device), attention_mask=input.attention_mask.to(device), output_hidden_states=True)\n",
    "\n",
    "    # Tuple of  (batch_size, sequence_length, hidden_size)\n",
    "    return output.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hidden_states(model, dataset:list, tokenizer, device):\n",
    "    result = {}\n",
    "\n",
    "    for data in tqdm(dataset):\n",
    "        hidden_states = get_hidden_states(model, data, tokenizer, device)\n",
    "\n",
    "        layer_num = model.config.num_hidden_layers\n",
    "        for layer_id in range(layer_num + 1):\n",
    "            vector_in_tensor = hidden_states[layer_id][0].to(\"cpu\")\n",
    "            vector_in_tensor = torch.mean(vector_in_tensor, dim=0)\n",
    "            try:\n",
    "                result[layer_id].append(vector_in_tensor)\n",
    "            except:\n",
    "                result[layer_id] = [vector_in_tensor]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 根据实际情况处理数据\n",
    "        sample = self.data[idx]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_prefix = {'en': '''It starts off a bit slow, but once the product placement jokes start it takes off.\n",
    "Emotion: \"positive\"             \n",
    "I've read this book with much expectation, it was very boring all through out the book\n",
    "Emotion: \"negative\"''',\n",
    "                 'zh': '''一开始有点慢，但一旦植入式笑话开始，它就会起飞。\n",
    "情感: \"积极\"\n",
    "我带着很大的期待读了这本书，整本书都很无聊\n",
    "情感: \"消极\"''',\n",
    "                  'hi': '''इसकी शुरुआत थोड़ी धीमी होती है, लेकिन एक बार जब उत्पाद प्लेसमेंट के बारे में मजाक शुरू हो जाता है तो इसमें तेजी आ जाती है।\n",
    "भावना: \"सकारात्मक\"\n",
    "मैंने यह किताब बहुत उम्मीद के साथ पढ़ी है, पूरी किताब में यह बहुत उबाऊ थी\n",
    "भावना: \"नकारात्मक\"''',\n",
    "                  'de':'''Es fängt etwas langsam an, aber sobald die Witze über die Produktplatzierung beginnen, geht es los.\n",
    "Emotion: \"positiv\"\n",
    "Ich habe dieses Buch mit großer Erwartung gelesen, es war durchweg sehr langweilig\n",
    "Emotion: \"negativ\"''',\n",
    "                  'fr':'''Cela commence un peu lentement, mais une fois que les blagues sur le placement de produit commencent, cela décolle.\n",
    "Émotion: \"positif\"\n",
    "J'ai lu ce livre avec beaucoup d'attente, c'était très ennuyeux tout au long du livre\n",
    "Émotion: \"négatif\"'''}\n",
    "\n",
    "emotion_map = {'en':'Emotion: ', 'zh':'情感: ', 'hi':'भावना: ', 'de':'Emotion: ', 'fr':'Émotion: '}\n",
    "\n",
    "\n",
    "def preprocess_data(data_path, language):\n",
    "    data_list = []\n",
    "    dataset = CustomDataset(data_path)\n",
    "    assert(dataset is not None)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    sample_iterator = tqdm(dataloader, desc=\"Inferenced batchs num\")\n",
    "    for step, inputs in enumerate(sample_iterator):\n",
    "        text_item = prompt_prefix[language] + '\\n' + inputs['input'][0] + '\\n' + emotion_map[language] +  '\\\"'\n",
    "        data_list.append(text_item)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_decomp(data:list, n_components=2, if_save=False, save_dir=None, model_size=None, layer=0):\n",
    "    \n",
    "    # labels = torch.cat([\n",
    "    #     torch.ones(100) * 1,\n",
    "    #     torch.ones(100) * 2,\n",
    "    #     torch.ones(100) * 3,\n",
    "    #     torch.ones(100) * 4,\n",
    "    #     # torch.ones(100) * 5,\n",
    "    #     # torch.ones(100) * 6,\n",
    "    #     # torch.ones(100) * 7,\n",
    "    #     # torch.ones(100) * 8,\n",
    "    #     # torch.ones(100) * 9,\n",
    "    #     # torch.ones(100) * 10,\n",
    "    # ], dim=0)\n",
    "\n",
    "    input_vec = torch.stack(tensors=data, dim=0)\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    pca_result = pca.fit_transform(input_vec.numpy())\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "\n",
    "    # plt.colorbar()\n",
    "\n",
    "    if if_save:\n",
    "        plt.scatter(pca_result[:500, 0], pca_result[:500, 1], label=\"en\")\n",
    "        plt.scatter(pca_result[500:1000, 0], pca_result[500:1000, 1], label=\"de\")\n",
    "        plt.scatter(pca_result[1000:1500, 0], pca_result[1000:1500, 1], label=\"fr\")\n",
    "        plt.scatter(pca_result[1500:, 0], pca_result[1500:, 1], label=\"hi\")\n",
    "\n",
    "        plt.legend(fontsize=16)\n",
    "        os.makedirs(f'{os.path.join(save_dir, model_size)}/PCA/emotion/2-shot/figures', exist_ok=True)\n",
    "        plt.savefig(f'{os.path.join(save_dir, model_size)}/PCA/emotion/2-shot/figures/{model_size}_pca_{layer}.pdf', dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.scatter(pca_result[:500, 0], pca_result[:500, 1], label=\"en\")\n",
    "        plt.scatter(pca_result[500:1000, 0], pca_result[500:1000, 1], label=\"de\")\n",
    "        plt.scatter(pca_result[1000:1500, 0], pca_result[1000:1500, 1], label=\"fr\")\n",
    "        plt.scatter(pca_result[1500:, 0], pca_result[1500:, 1], label=\"hi\")\n",
    "\n",
    "        plt.legend(fontsize=16)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_decomp_seperate(data:list, n_components=2, if_save=False, save_dir=None, model_size=None, layer=0):\n",
    "    \n",
    "    # labels = torch.cat([\n",
    "    #     torch.ones(100) * 1,\n",
    "    #     torch.ones(100) * 2,\n",
    "    #     torch.ones(100) * 3,\n",
    "    #     torch.ones(100) * 4,\n",
    "    #     # torch.ones(100) * 5,\n",
    "    #     # torch.ones(100) * 6,\n",
    "    #     # torch.ones(100) * 7,\n",
    "    #     # torch.ones(100) * 8,\n",
    "    #     # torch.ones(100) * 9,\n",
    "    #     # torch.ones(100) * 10,\n",
    "    # ], dim=0)\n",
    "\n",
    "    input_vec = torch.stack(tensors=data, dim=0)\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    pca_result = pca.fit_transform(input_vec.numpy())\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    # print((pca_result[:10, 0]).type)\n",
    "\n",
    "\n",
    "    # plt.colorbar()\n",
    "\n",
    "    if if_save:\n",
    "        plt.scatter(np.concatenate((pca_result[:100*1, 0], pca_result[500:500+100*1, 0], pca_result[1000:1000+100*1, 0], pca_result[1500:1500+100*1, 0])), np.concatenate((pca_result[:100*1, 1], pca_result[500:500+100*1, 1], pca_result[1000:1000+100*1, 1], pca_result[1500:1500+100*1, 1])), label=\"0-100\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*1:100*2, 0], pca_result[500+100*1:500+100*2, 0], pca_result[1000+100*1:1000+100*2, 0], pca_result[1500+100*1:1500+100*2, 0])), np.concatenate((pca_result[100*1:100*2, 1], pca_result[500+100*1:500+100*2, 1], pca_result[1000+100*1:1000+100*2, 1], pca_result[1500+100*1:1500+100*2, 1])), label=\"100-200\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*2:100*3, 0], pca_result[500+100*2:500+100*3, 0], pca_result[1000+100*2:1000+100*3, 0], pca_result[1500+100*2:1500+100*3, 0])), np.concatenate((pca_result[100*2:100*3, 1], pca_result[500+100*2:500+100*3, 1], pca_result[1000+100*2:1000+100*3, 1], pca_result[1500+100*2:1500+100*3, 1])), label=\"200-300\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*3:100*4, 0], pca_result[500+100*3:500+100*4, 0], pca_result[1000+100*3:1000+100*4, 0], pca_result[1500+100*3:1500+100*4, 0])), np.concatenate((pca_result[100*3:100*4, 1], pca_result[500+100*3:500+100*4, 1], pca_result[1000+100*3:1000+100*4, 1], pca_result[1500+100*3:1500+100*4, 1])), label=\"300-400\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*4:100*5, 0], pca_result[500+100*4:500+100*5, 0], pca_result[1000+100*4:1000+100*5, 0], pca_result[1500+100*4:1500+100*5, 0])), np.concatenate((pca_result[100*4:100*5, 1], pca_result[500+100*4:500+100*5, 1], pca_result[1000+100*4:1000+100*5, 1], pca_result[1500+100*4:1500+100*5, 1])), label=\"400-500\")\n",
    "\n",
    "        plt.legend(fontsize=16)\n",
    "        os.makedirs(f'{os.path.join(save_dir, model_size)}/PCA/emotion/2-shot/figures', exist_ok=True)\n",
    "        plt.savefig(f'{os.path.join(save_dir, model_size)}/PCA/emotion/2-shot/figures/{model_size}_pca_{layer}.pdf', dpi=300, bbox_inches='tight')\n",
    "    else:\n",
    "        plt.scatter(np.concatenate((pca_result[:100*1, 0], pca_result[500:500+100*1, 0], pca_result[1000:1000+100*1, 0], pca_result[1500:1500+100*1, 0])), np.concatenate((pca_result[:100*1, 1], pca_result[500:500+100*1, 1], pca_result[1000:1000+100*1, 1], pca_result[1500:1500+100*1, 1])), label=\"0-100\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*1:100*2, 0], pca_result[500+100*1:500+100*2, 0], pca_result[1000+100*1:1000+100*2, 0], pca_result[1500+100*1:1500+100*2, 0])), np.concatenate((pca_result[100*1:100*2, 1], pca_result[500+100*1:500+100*2, 1], pca_result[1000+100*1:1000+100*2, 1], pca_result[1500+100*1:1500+100*2, 1])), label=\"100-200\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*2:100*3, 0], pca_result[500+100*2:500+100*3, 0], pca_result[1000+100*2:1000+100*3, 0], pca_result[1500+100*2:1500+100*3, 0])), np.concatenate((pca_result[100*2:100*3, 1], pca_result[500+100*2:500+100*3, 1], pca_result[1000+100*2:1000+100*3, 1], pca_result[1500+100*2:1500+100*3, 1])), label=\"200-300\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*3:100*4, 0], pca_result[500+100*3:500+100*4, 0], pca_result[1000+100*3:1000+100*4, 0], pca_result[1500+100*3:1500+100*4, 0])), np.concatenate((pca_result[100*3:100*4, 1], pca_result[500+100*3:500+100*4, 1], pca_result[1000+100*3:1000+100*4, 1], pca_result[1500+100*3:1500+100*4, 1])), label=\"300-400\")\n",
    "        plt.scatter(np.concatenate((pca_result[100*4:100*5, 0], pca_result[500+100*4:500+100*5, 0], pca_result[1000+100*4:1000+100*5, 0], pca_result[1500+100*4:1500+100*5, 0])), np.concatenate((pca_result[100*4:100*5, 1], pca_result[500+100*4:500+100*5, 1], pca_result[1000+100*4:1000+100*5, 1], pca_result[1500+100*4:1500+100*5, 1])), label=\"400-500\")\n",
    "\n",
    "        plt.legend(fontsize=16)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_decomp_pearson(data:list, n_components=1, model_size=None, layer=0):\n",
    "    \n",
    "    print(f\"Layer is {layer}\")\n",
    "\n",
    "    # labels = torch.cat([\n",
    "    #     torch.ones(100) * 1,\n",
    "    #     torch.ones(100) * 2,\n",
    "    #     torch.ones(100) * 3,\n",
    "    #     torch.ones(100) * 4,\n",
    "    #     # torch.ones(100) * 5,\n",
    "    #     # torch.ones(100) * 6,\n",
    "    #     # torch.ones(100) * 7,\n",
    "    #     # torch.ones(100) * 8,\n",
    "    #     # torch.ones(100) * 9,\n",
    "    #     # torch.ones(100) * 10,\n",
    "    # ], dim=0)\n",
    "\n",
    "    input_vec = torch.stack(tensors=data, dim=0)\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    pca_result = pca.fit_transform(input_vec.numpy())\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    # print((pca_result[:10, 0]).type)\n",
    "\n",
    "\n",
    "    # plt.colorbar()\n",
    "\n",
    "    en = pca_result[:500, 0]\n",
    "    de = pca_result[500:1000, 0]\n",
    "    fr = pca_result[1000:1500, 0]\n",
    "    hi = pca_result[1500:, 0]\n",
    "\n",
    "    pearson_ende = np.corrcoef(en, de)[0, 1]\n",
    "    pearson_enfr = np.corrcoef(en, fr)[0, 1]\n",
    "    pearson_enhi = np.corrcoef(en, hi)[0, 1]\n",
    "    pearson_defr = np.corrcoef(de, fr)[0, 1]\n",
    "    pearson_dehi = np.corrcoef(de, hi)[0, 1]\n",
    "    pearson_frhi = np.corrcoef(fr, hi)[0, 1]\n",
    "\n",
    "    print(f\"pearson_ende: {pearson_ende}\")\n",
    "    print(f\"pearson_enfr: {pearson_enfr}\")\n",
    "    print(f\"pearson_enhi: {pearson_enhi}\")\n",
    "    print(f\"pearson_defr: {pearson_defr}\")\n",
    "    print(f\"pearson_dehi: {pearson_dehi}\")\n",
    "    print(f\"pearson_frhi: {pearson_frhi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.41s/it]\n",
      "Inferenced batchs num: 100%|██████████| 500/500 [00:00<00:00, 45379.15it/s]\n",
      "100%|██████████| 500/500 [02:12<00:00,  3.77it/s]\n",
      "Inferenced batchs num: 100%|██████████| 500/500 [00:00<00:00, 46810.38it/s]\n",
      "100%|██████████| 500/500 [02:21<00:00,  3.53it/s]\n",
      "Inferenced batchs num: 100%|██████████| 500/500 [00:00<00:00, 41195.75it/s]\n",
      "100%|██████████| 500/500 [02:20<00:00,  3.55it/s]\n",
      "Inferenced batchs num: 100%|██████████| 500/500 [00:00<00:00, 33028.10it/s]\n",
      "100%|██████████| 500/500 [03:22<00:00,  2.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model_size = 'mistral-7b'\n",
    "\n",
    "all_avaliable_model = {'llama2-7b':'/home/nfs02/model/llama2/hf/Llama-2-7b-hf', 'llama2-13b':'/home/nfs02/model/llama2/hf/meta-llama_Llama-2-13b-hf', \n",
    "                       'llama2-7b-aligned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/llama2_zhjahi3k_round1_epoch12',\n",
    "                       'llama2-13b':'/home/nfs02/llama2/hf/Llama-2-13b-hf',\n",
    "                       'llama3-8b':'/home/nfs02/model/llama-3-8b',\n",
    "                       'llama3-8b-aligned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/llama3-8b_emotion_zhes20k_round1_epoch3',\n",
    "                       'mistral-7b':'/home/nfs02/model/mistralai_Mistral-7B-v0.1',\n",
    "                       'mistral-7b-instructv0.2':'/home/nfs02/model/Mistral-7B-Instruct-v0.2',\n",
    "                       'mistral-7b-aligned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/mistral_swhi20k_round1_epoch3',\n",
    "                       'mistral-7b-tuned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/mistral_round2_epoch3',\n",
    "                       'mistral-7b-tuned-aligned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/mistral_zhswgdbe40k_round12_epoch3',\n",
    "                       'Qwen1.5-4b':'/home/nfs02/model/Qwen1.5-4B',\n",
    "                       'Qwen1.5-4b-aligned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/Qwen4b_emotion_swbe20k_round1_epoch3',\n",
    "                       'Qwen1.5-14b':'/home/nfs02/model/Qwen1.5-14B-Base',\n",
    "                       'Qwen1.5-14b-aligned':'/home/nfs03/zhangsm/multiL-transfer-interpretability/pretrained-models/Qwen14b_emotion_zhde20k_round1_epoch3'}\n",
    "model_name_or_path = all_avaliable_model[model_size]\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_auth_token=None)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, use_auth_token=None, device_map='auto', quantization_config=quantization_config)\n",
    "# model.to(device)\n",
    "hidden_states = {}\n",
    "\n",
    "\n",
    "for language in [\"en\", \"de\", \"fr\", \"hi\"]:\n",
    "    data_path = \"/home/nfs03/zhangsm/multiL-transfer-interpretability/zhangsm-multiL/data/ap_emotion/emotion_{}500_test.json\".format(language)\n",
    "    data_list = preprocess_data(data_path, language)\n",
    "\n",
    "    hidden_states[language] = get_all_hidden_states(model, data_list, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer is 32\n",
      "pearson_ende: 0.6608538878146463\n",
      "pearson_enfr: 0.728393729798287\n",
      "pearson_enhi: -0.06737416180488459\n",
      "pearson_defr: 0.6962148828916844\n",
      "pearson_dehi: -0.08623824739569996\n",
      "pearson_frhi: -0.151507368148133\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get pca result\n",
    "# layer = 32\n",
    "# input_tensor = [\n",
    "#     item for lang in [\"en\", \"de\", \"fr\", \"hi\"] for item in hidden_states[lang][layer][:500]\n",
    "# ]\n",
    "\n",
    "# pca_decomp(input_tensor, if_save=False, save_dir='../zsm-results', model_size=model_size, layer=layer)\n",
    "\n",
    "\n",
    "# for layer in range(0, 41):\n",
    "#     input_tensor = [\n",
    "#         item for lang in [\"en\", \"de\", \"fr\", \"hi\"] for item in hidden_states[lang][layer][:500]\n",
    "#     ]\n",
    "\n",
    "#     pca_decomp(input_tensor, if_save=True, save_dir='./zsm-results', model_size=model_size, layer=layer)\n",
    "\n",
    "\n",
    "# layer = 20\n",
    "# input_tensor = [\n",
    "#     item for lang in [\"en\", \"de\", \"fr\", \"hi\"] for item in hidden_states[lang][layer][:500]\n",
    "# ]\n",
    "\n",
    "# pca_decomp_seperate(input_tensor, if_save=False, save_dir='../zsm-results', model_size=model_size, layer=layer)\n",
    "\n",
    "\n",
    "layer = 32\n",
    "input_tensor = [\n",
    "    item for lang in [\"en\", \"de\", \"fr\", \"hi\"] for item in hidden_states[lang][layer][:500]\n",
    "]\n",
    "\n",
    "pca_decomp_pearson(input_tensor, n_components=1, model_size=model_size, layer=layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## locating language nerual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_one_nerual(theta):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caoyq_alpaca_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
